{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfolding Black Box Learning\n",
    "## Seeing the World from the Perspective of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, Harman demonstrated the fundamentals of a neural network. We were introduced to concepts like \"activation function\", \"gradient descent\", and \"backpropagation\" and saw how it came together to recognize hand-written digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are looking at what is happening between the input layer and the output layer of the neural network: we are going to understand what exactly those hidden layers are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NeuralNetwork.png\" width=\"500px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(300, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97        88\n",
      "           1       0.95      0.90      0.93        91\n",
      "           2       0.96      1.00      0.98        86\n",
      "           3       0.98      0.87      0.92        91\n",
      "           4       0.99      0.92      0.96        92\n",
      "           5       0.89      0.96      0.92        91\n",
      "           6       0.98      0.99      0.98        91\n",
      "           7       0.95      0.99      0.97        89\n",
      "           8       0.92      0.92      0.92        88\n",
      "           9       0.88      0.95      0.91        92\n",
      "\n",
      "    accuracy                           0.94       899\n",
      "   macro avg       0.95      0.94      0.94       899\n",
      "weighted avg       0.95      0.94      0.94       899\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's use the Sci-Kit Learn Library to Program this Example!\n",
    "# Code from: https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scripts import *\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "images, labels, n_samples, data = preprocess(plt, digits.images, digits.target)\n",
    "\n",
    "# MLP = Multi-Layer Perceptron = Neural Network\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(300, 100))\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], labels[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = labels[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[83  0  0  1  1  0  3  0  0  0]\n",
      " [ 0 79  1  1  2  1  2  0  1  4]\n",
      " [ 1  2 78  4  0  0  0  0  0  1]\n",
      " [ 0  0  3 81  0  2  0  3  2  0]\n",
      " [ 2  1  0  0 86  0  1  0  2  0]\n",
      " [ 0  0  0  2  0 87  2  0  0  0]\n",
      " [ 0  1  0  0  0  0 90  0  0  0]\n",
      " [ 0  2  0  1  1  0  0 81  1  3]\n",
      " [ 0  7  1  1  1  3  0  0 71  4]\n",
      " [ 2  0  0  2  0  6  0  0  1 81]]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the confusion matrix:\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ConfusionMatrix.png\" width=\"500px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the intuition behind these hidden layers? Why do we need them at all?\n",
    "\n",
    "Consider what we see when we see a number. We don't see the individual pixels - that kind of information is too dense for us to do anything useful with. Instead, we see higher-order patterns: lines and edges.\n",
    "So, what if instead of traing a network to recognize digits, we train it to recognizes lines and edges first.\n",
    "Then we use the lines and edges to predict numbers.\n",
    "\n",
    "This is exactly what the neural network is doing!\n",
    "\n",
    "<img src=\"images/HiddenLayers.gif\" width=\"500px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this effect in <a target=\"_blank\" href=\"http://scs.ryerson.ca/~aharley/vis/\">action</a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my view, \"hidden layer\" is a bad term - it doesn't get acrosss any of the intuition as to what hidden layers are doing, or why they work! From this point on we will call these layers **latent factor models**, because the job of each layer to to build a model that represents the higher-order components (\"latent factors\") for the next layer (whether the next layer is building another latent factor model, or if the next layer is the output layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, when we visualize these hidden layers, we find a latent-factor model that is *meaningful* to us, like in the convolutional neural network case in the visualization example. But sometimes, the latent-factor model computed is only vaguely familar to the object it is trying to recognize, like in image classification:\n",
    "\n",
    "<img src=\"images/CIFAR-10-LFM.png\" width=\"750px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these models are reasonable: an airplane is usually pictured with a blue sky above, a ship is pictured with a blue sea below, a frog is very green in the center, etc. But for the most part, these latent factor models are fairly opaque to us. Even still, they definitely aren't \"hidden\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other reason the term \"hidden layer\" is a poor description of what is going on is that it assumes that these layers are meaningless to us - that all we care about are input and output layers. This couldn't be further from the truth. In fact, the latent factor models generated from a neural network unlock the ability to use machine learning beyond the usual classification, regression, or clustering tasks. Using these latent factor models, we can generate new outputs using only these latent factor representation via **autoencoders**.\n",
    "\n",
    "<img src=\"images/Autoencoder.png\" width=\"475px\" style=\"float: left\" />\n",
    "<img src=\"images/MNIST-Autoencoder.png\" width=\"500px\" style=\"position: relative; float: right; top: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder using the latent factor representation to generate new outputs. To visualize this, imagine taking the MNIST neural network example and running it backwards - that is, given a number from 0 to 9, draw it on a two dimensional plane. This sounds difficult, so maybe instead of representing from the output space you represent it using one the latent factor models, manually setting each of the nodes until an interesting image comes out on the other end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with anything in machine learning, the best way to explain it is <a href=\"http://nvidia-research-mingyuliu.com/gaugan/\" target=\"_blank\">by example</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we use an auto-encoder on MNIST hand-written digits?\n",
    "\n",
    "First we to understand the architecture:\n",
    "(from https://gertjanvandenburg.com/blog/autoencoder/)\n",
    "\n",
    "<img src=\"images/MNIST-Autoencoder-Nodes.png\" width=\"500px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input image (green) is sent through two \"hidden layers\" (AKA the encoder, in blue). The two yellow nodes represent the \"Compressed Feature Vector\" - that is, the latent-factor model we will use to reconstruct/generate hand-written digits.\n",
    "\n",
    "Then, when we reconstruct each image by using the Compressed Feature Vector to pass through two extra hidden layers (AKA the decoder, in blue on the right) to get to the reconstructed image (AKA the output, in red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of this is mind-blowing: we can represent using **two** real numbers! Using these numbers, we can graph the input space like so:\n",
    "\n",
    "<img src=\"images/MNIST-Latent-Factor-Space.png\" width=\"500px\" style=\"float: left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/MNIST-Latent-Circle.mp4\" controls  width=\"750\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not only can we represent hand-written digits we have already seen using these numbers, \n",
    "# but we can also generate new digits based on these numbers:\n",
    "from IPython.display import Video\n",
    "\n",
    "Video(\"videos/MNIST-Latent-Circle.mp4\", width=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for this session! To recap, we covered:\n",
    "- An intuition for hidden layers in a neural network\n",
    "- Why \"hidden layer\" is a terrible term, and we should really say \"latent factor model\"\n",
    "- How to visualize latent factor models\n",
    "- How to use latent factor models to generate output using auto-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
